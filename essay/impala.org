* impala
  - github http://github.com/cloudera/impala
  - documentation https://ccp.cloudera.com/display/IMPALA10BETADOC/Cloudera+Impala+1.0+Beta+Documentation
  - Parquet: Columnar Storage for Hadoop http://parquet.github.com/
  - Introducing Parquet: Efficient Columnar Storage for Apache Hadoop | Cloudera Developer Blog : http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/
  - External Hands-on Experiences with Cloudera Impala | Apache Hadoop for the Enterprise | Cloudera http://blog.cloudera.com/blog/2012/11/external-observations-about-cloudera-impala/
  - Cloudera Impala発表資料 | 外道父の匠 : http://blog.father.gedow.net/2012/11/27/cloudera-impala-presentation/

** Cloudera Impala: Real-Time Queries in Apache Hadoop, For Real | Apache Hadoop for the Enterprise | Cloudera
http://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/

   - With Impala, you can query data, whether stored in HDFS or Apache HBase – including SELECT, JOIN, and aggregate functions – in real time. (数据存储在HDFS以及HBase）
   - Furthermore, it uses the same metadata, SQL syntax (Hive SQL), ODBC driver and user interface (Hue Beeswax) as Apache Hive, providing a familiar and unified platform for batch-oriented or real-time queries. (For that reason, Hive users can utilize Impala with little setup overhead.) （使用了和Hive相同的metadata，SQL语法以及ODBC driver以及user interface，这样就为real-time以及batch query提供了统一的平台，对于Hive用户来说学习代价非常小）
   - The first beta drop includes support for text files and SequenceFiles; SequenceFiles can be compressed as Snappy, GZIP, and BZIP (with Snappy recommended for maximum performance). Support for additional formats including Avro, RCFile, LZO text files, and Doug Cutting’s Trevni columnar format is planned for the production drop.（beta版本支持在HDFS上面存储text以及sequenceFile，SF可以使用snappy，gzip以及bzip压缩。产品级别释出的时候会支持一些其他的格式比如Avro，RCFile，LZO txt以及Doug Cutting编写的Trevni的列式存储格式）
   - To avoid latency, Impala circumvents MapReduce to directly access the data through a specialized distributed query engine that is very similar to those found in commercial parallel RDBMSs.（为了缩短延迟，impala绕过mr，通过类似商业并行的RDBMS的分布式查询引擎来访问数据）
   - Unlike Dremel as described in the 2010 paper, which could only handle single-table queries, Impala already supports the full set of join operators that are one of the factors that make SQL so popular.（dremel只是支持在一个table上面的query，而impala可以支持SQL里面所有的join操作）

file:./images/impala-high-level-architecture.png

** Cloudera+Impala+1.0+Beta+Documentation
*** How Cloudera Impala Works with CDH
The Impala solution is composed of the following components:
   - Clients - Entities including Hue, ODBC clients, JDBC clients, and the Impala Shell can all interact with Impala. These interfaces are typically used to issue queries or complete administrative tasks such as connecting to Impala.  (通过hue,odbc,jdcb或者是shell来和impala做交互，发起交互或者是管理命令）
   - Hive Metastore - Stores information about the data available to Impala. For example, the metastore lets Impala know what databases are available and what the structure of those databases is.（hive metastore用来存储元数据）
   - Cloudera Impala - This process, which runs on datanodes, coordinates and executes queries. Each instance of Impala can receive, plan, and coordinate queries from Impala clients. Queries are distributed among Impala nodes, and these nodes then act as workers, executing parallel query fragments.
   - HBase and HDFS - Storage for data to be queried.

Queries executed using Impala are handled as follows:
   1. User applications send SQL queries to Impala through ODBC or JDBC, which provide standardized querying interfaces. The user application may connect to any impalad in the cluster. This impalad becomes the coordinator for the query. （通过connecor发送到集群中任意的impalad）
   2. Impala parses the query and analyzes it to determine what tasks need to be performed by impalad instances across the cluster. Execution is planned for optimal efficiency.（query planner进行查询规划，并且通过query coordinator调度到其他机器上的impalad)
   3. Services such as HDFS and HBase are accessed by local impalad instances to provide data.(query exec engine访问hdfs/hbase数据） *note(dirlt): 但是应该考虑了locality*
   4. Each impalad returns data to the coordinating impalad, which sends these results to the client. 

** Installation
*** Hardware Requirements
   - During join operations all data from both data sets is loaded into memory. Data sets can be very large, so ensure your hardware has sufficient memory to accommodate the joins you anticipate completing. *note(dirlt):这有点汗，join操作都是在全内存完成的*
   - CPU - Impala uses the SSE4.2 instruction set, which is included in newer processors. Impala can use older processors, but for best performance use:
     - Intel - Nehalem (released 2008) or later processors.
     - AMD - Bulldozer (released 2011) or later processors.
   - Memory - 32GB or more. Impala cannot run queries that have a working set greater than the total available ram. Note that the working set is not the size of the input.
   - Storage - DataNodes with 10 or more disks each. I/O speeds are often the limiting factor for disk performance with Impala. Ensure you have sufficient disk space to store the data Impala will be querying. *note(dirlt)：感叹未来的趋势就是拿内存当磁盘用*

*** Prerequsite
   - Install CDH4 as described in CDH4 Installation. https://ccp.cloudera.com/display/CDH4DOC/CDH4+Installation
     - 添加下面source,然后sudo apt-get update
     - sudo apt-get install hadoop-0.20-mapreduce-jobtracker
     - sudo apt-get install hadoop-hdfs-namenode
     - sudo apt-get install hadoop-0.20-mapreduce-tasktracker 
     - sudo apt-get install hadoop-hdfs-datanode
     - sudo apt-get install hadoop-client
   - Install Hive as described in Hive Installation. As part of this process, you must configure Hive to use an external database as a metastore. 必须使用外部数据库来作为metastore *note(dirlt):hive允许使用内嵌数据库做metastore*
     - sudo apt-get install hive
#+BEGIN_EXAMPLE
deb [arch=amd64] http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh precise-cdh4 contrib
deb-src http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh precise-cdh4 contrib

deb [arch=amd64] http://beta.cloudera.com/impala/ubuntu/precise/amd64/impala precise-impala0 contrib
deb-src http://beta.cloudera.com/impala/ubuntu/precise/amd64/impala precise-impala0 contrib
#+END_EXAMPLE

tarball:
   - hadoop http://archive.cloudera.com/cdh4/cdh/4/hadoop-2.0.0-cdh4.2.0.tar.gz
   - hbase http://archive.cloudera.com/cdh4/cdh/4/hbase-0.94.2-cdh4.2.0.tar.gz
   - zookeeper http://archive.cloudera.com/cdh4/cdh/4/zookeeper-3.4.5-cdh4.2.0.tar.gz
   - hive http://archive.cloudera.com/cdh4/cdh/4/hive-0.10.0-cdh4.2.0.tar.gz

*note(dirlt):最好不要用tarball安装* impala需要使用hdfs的short-circuit read的特性，这个特性需要有libhadoop.so.但是tarball没有自带native实现
#+BEGIN_EXAMPLE
Enabling short-circuit reads allows Impala to read local data directly from the file system. This removes
the need to communicate through the DataNodes, improving performance. This setting also minimizes
the number of additional copies of data. Short-circuit reads requires libhadoop.so (the Hadoop
Native Library) to be accessible to both the server and the client. libhadoop.so is not available if you
have installed from a tarball. You must install from an .rpm, .deb, or parcel in order to use short-circuit
local reads.
#+END_EXAMPLE

*** Ubuntu
source compile: *note(dirlt)：比较麻烦*
   - sudo apt-get install libboost-all-dev libevent1-dev automake libtool flex bison g++ libssl-dev make cmake doxygen libglib2.0-dev python-dev libzip2 subversion libsasl2-dev wget git unzip
   - llvm maven3 # github README上面有描述
   - git clone https://github.com/cloudera/impala.git
   - cd impala && . bin/impala-config.sh # *note(dirlt):我使用oh-my-zsh似乎有问题，切换成为bash就成功*
   - cd thirdparty && ./download_thirdparty.sh # *note(dirlt):如果自己手动下载了hadoop和hive的话，可以修改一下脚本，因为这两个包还是比较大的*
   - cd ${IMPALA_HOME} && ./build_public.sh -build_thirdparty
   - *note(dirlt)：编译麻烦，我没有成功，放弃*

binary install:
   - sudo apt-get install impala
   - sudo apt-get install impala-shell

*** Gettting Started
   - 启动hdfs
   - 无需启动mapreduce/yarn/hbase.
   - 启动hive metastore
   - 使用hive创建table并且导入数据
   - 启动impalad # impala daemon. sudo impalad start
   - 启动statstored # imapala存储统计数据进行优化. sudo statestored start
   - 启动impala shell 
     - connect <host> # 连接到host的impalad. connect localhost
     - refresh # 从hive metastore读取meta数据，保存在内存中
     - SQL语句

下面是一个例子，使用Hive和Impala来做SQL查询

#+BEGIN_EXAMPLE
➜  lib  impala-shell
Welcome to the Impala shell. Press TAB twice to see a list of available commands.

Copyright (c) 2012 Cloudera, Inc. All rights reserved.

(Build version: Impala v0.6 (720f93c) built on Sat Feb 23 18:52:43 PST 2013)
[Not connected] > connect localhost
Connected to localhost:21000
[localhost:21000] > refresh
Successfully refreshed catalog
[localhost:21000] > select * from kv where k = 400;
Query: select * from kv where k = 400
Query finished, fetching results ...
400	val_400
Returned 1 row(s) in 0.65s
[localhost:21000] > 
#+END_EXAMPLE


*** TroubleShooting
-----
启动sudo impalad start出现下面错误
#+BEGIN_EXAMPLE
0314 16:41:13.884233 18187 impala-server.cc:573] ERROR: short-circuit local reads is disabled because
  - dfs.client.read.shortcircuit is not enabled.
E0314 16:41:13.884558 18187 impala-server.cc:575] Impala is aborted due to improper configurations.
#+END_EXAMPLE

这个问题原因是因为impala需要使用hdfs的short-circuit功能直接读取本地文件系统，避免从datannode传输。为了使用这个功能需要在hdfs-site.xml加上下面选项
#+BEGIN_EXAMPLE
<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
</property>
<property>
  <name>dfs.domain.socket.path</name>
  <value>/var/run/hadoop-hdfs/dn._PORT</value>
</property>
#+END_EXAMPLE
