* hive
#+OPTIONS: H:5
  - Home - Apache Hive - Apache Software Foundation https://cwiki.apache.org/confluence/display/Hive/Home

** Getting Started
   - https://cwiki.apache.org/confluence/display/Hive/GettingStarted

首先配置hdfs/mapred/hbase,然后配置hive.过程不是很麻烦。

*** Configuration
   - 默认配置文件是 <install-dir>/conf/hive-default.xml
   - 可以通过配置文件改写 <install-dir>/conf/hive-site.xml
   - 配置文件路径 HIVE_CONF_DIR
   - 日志配置文件 <install-dir>/conf/hive-log4j.properties
   - 启动通过 bin/hive -hiveconf x1=y1 -hiveconf x2=y2 设置参数
   - 运行时候通过 SET mapred.job.tracker=myhost.mycompany.com:50030; 修改参数

*** Local Mode
这里所谓的local-mode主要是指运行的mapreduce是在local node上面完成的，至于数据源还是和hdfs/hbase本身配置相关。可以通过设置 SET mapred.job.tracker=local; 强制修改mapreduce本地完成。

hive0.7以后提供自动切换local-mode功能，设置 hive> SET hive.exec.mode.local.auto=false; 那么对于下面三个情况满足的条件下就会自动切换到local-mode:
   - The total input size of the job is lower than: hive.exec.mode.local.auto.inputbytes.max (128MB by default)
   - The total number of map-tasks is less than: hive.exec.mode.local.auto.tasks.max (4 by default)
   - The total number of reduce tasks required is 1 or 0.

*** Metadata Store
    - https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin *note(dirlt)：配置remote server*

metadata store主要是用来存储数据库的一些元数据信息，有下面相关的配置参数：
   - javax.jdo.option.ConnectionURL.
   - javax.jdo.option.ConnectionDriverName.
默认实现是在本地的derby db，默认存储位置是./metastore_db. metastore其他实现需要支持JPO（Java Persistent Object） Metastore can be stored in any database that is supported by JPOX. The database schema is defined in JDO metadata annotations file package.jdo at src/contrib/hive/metastore/src/model.

-----

*配置remote server*

remote metadata store server和client之间交互是通过thrift完成的，thrift server通过jdbc连接到mysql或者是其他数据库上。 *note(dirlt):之所以需要这个主要是因为impala要求metastore必须是remote的*

Server Configuration Parameters
| Config Param                          | Config Value                                                           | Comment                                  |
|---------------------------------------+------------------------------------------------------------------------+------------------------------------------|
| javax.jdo.option.ConnectionURL        | jdbc:mysql://<host name>/<database name>?createDatabaseIfNotExist=true | metadata is stored in a MySQL server     |
| javax.jdo.option.ConnectionDriverName | com.mysql.jdbc.Driver                                                  | MySQL JDBC driver class                  |
| javax.jdo.option.ConnectionUserName   | <user name>                                                            | user name for connecting to mysql server |
| javax.jdo.option.ConnectionPassword   | <password>                                                             | password for connecting to mysql server  |
| hive.metastore.warehouse.dir          | <base hdfs path>                                                       | default location for Hive tables.        |

If you are using MySQL as the datastore for metadata, put MySQL client libraries in HIVE_HOME/lib before starting Hive Client or HiveMetastore Server. 如果使用ubuntu的话，可以直接使用 sudo apt-get install libmysql-java 安装，然后jar都在/usr/share/java下面。

Client Configuration Parameters
| Config Param                 | Config Value                | Comment                                       |
|------------------------------+-----------------------------+-----------------------------------------------|
| hive.metastore.uris          | thrift://<host_name>:<port> | host and port for the thrift metastore server |
| hive.metastore.local         | false                       | this is local store                           |
| hive.metastore.warehouse.dir | <base hdfs path>            | default location for Hive tables.             |

thrift server 通过 hive --service metastore 启动，port在9083上面. 环境变量METASTORE_PORT来配置。 *note(dirlt)：hive-env.sh里面可以设置*
#+BEGIN_EXAMPLE
13/03/07 18:06:34 INFO metastore.HiveMetaStore: Started the new metaserver on port [9083]...
13/03/07 18:06:34 INFO metastore.HiveMetaStore: Options.minWorkerThreads = 200
13/03/07 18:06:34 INFO metastore.HiveMetaStore: Options.maxWorkerThreads = 100000
13/03/07 18:06:34 INFO metastore.HiveMetaStore: TCP keepalive = true
#+END_EXAMPLE

*** Example
   - 数据默认是使用ctrl-a来做分割 *note(dirlt)：how wierd:(*

#+BEGIN_EXAMPLE
➜  bin  hadoop fs -copyFromLocal ../examples/files/kv1.txt /tmp/
13/03/07 14:34:40 INFO security.UserGroupInformation: JAAS Configuration already set up for Hadoop, not re-installing.
➜  bin  hive
Hive history file=/tmp/dirlt/hive_job_log_dirlt_201303071434_1408198373.txt
hive> DROP TABLE kv;
OK
Time taken: 4.647 seconds
hive> CREATE TABLE kv (k INT,v STRING);
OK
Time taken: 0.201 seconds
hive> LOAD DATA INPATH '/tmp/kv1.txt' OVERWRITE INTO TABLE kv;
Loading data to table default.kv
Moved to trash: hdfs://localhost:9000/home/dirlt/hive/warehouse/kv
OK
Time taken: 0.225 seconds
hive> SELECT * from kv WHERE k = 417; 
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201303071324_0006, Tracking URL = http://localhost:50030/jobdetails.jsp?jobid=job_201303071324_0006
Kill Command = /home/dirlt/utils/hadoop-0.20.2-cdh3u3//bin/hadoop job  -Dmapred.job.tracker=localhost:9001 -kill job_201303071324_0006
2013-03-07 14:36:14,960 Stage-1 map = 0%,  reduce = 0%
2013-03-07 14:36:16,970 Stage-1 map = 100%,  reduce = 0%
2013-03-07 14:36:17,982 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201303071324_0006
OK
417     val_417
417     val_417
417     val_417
Time taken: 5.787 seconds
#+END_EXAMPLE
   - copy to hdfs
   - create table.
   - load data.
   - do select *note(dirlt):可以看到这里运行了mr任务*


*** TroubleShooting
--------------------
调用SQL select count(*) from kv, hive出现下面问题（CDH4.2.0有，CDH3u3没有）
#+BEGIN_EXAMPLE
java.io.FileNotFoundException: File does not exist: hdfs://localhost:9000/home/dirlt/utils/hive-0.10.0-cdh4.2.0/lib/hive-builtins-0.10.0-cdh4.2.0.jar
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:807)
	at org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.getFileStatus(ClientDistributedCacheManager.java:288)
	at org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.getFileStatus(ClientDistributedCacheManager.java:224)
	at org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.determineTimestamps(ClientDistributedCacheManager.java:93)
	at org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(ClientDistributedCacheManager.java:57)
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:254)
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:290)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:361)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:617)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:612)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:612)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:447)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Job Submission failed with exception 'java.io.FileNotFoundException(File does not exist: hdfs://localhost:9000/home/dirlt/utils/hive-0.10.0-cdh4.2.0/lib/hive-builtins-0.10.0-cdh4.2.0.jar)'
#+END_EXAMPLE

这个问题我猜想应该Hive CDH4之后使用lean jar的方式，要求依赖jar文件存放在hdfs上面，运行时候然后下载到本地执行。把hive/lib下面的jar全部上传到HDFS上面之后这个问题解决。
